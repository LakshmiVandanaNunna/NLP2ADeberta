# -*- coding: utf-8 -*-
"""Sentence Entailment Deberta.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xZ_oR2O-9sNliaIlbsbAsZyaYIJ6J7fX
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers
!pip install sentencepiece
# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl

import pandas as pd
import re
import torch
# import torch_xla
# import torch_xla.core.xla_model as xm
from torch.utils.data import Dataset, TensorDataset, DataLoader, SequentialSampler, RandomSampler
from torch.nn.utils.rnn import pad_sequence
# from keras.preprocessing.sequence import pad_sequences
import pickle
import os
from transformers import DebertaTokenizer
#from transformers import AlbertTokenizer
import numpy as np

# device = xm.xla_device()
# print(device)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

train_df = pd.read_csv('/content/drive/My Drive/train_data.csv')
val_df   = pd.read_csv('/content/drive/My Drive/val_data.csv')

train_df = train_df.dropna()
val_df = val_df.dropna()

train_df['sentence1'] = train_df['sentence1'].astype(str)
train_df['sentence2'] = train_df['sentence2'].astype(str)

val_df['sentence1'] = val_df['sentence1'].astype(str)
val_df['sentence2'] = val_df['sentence2'].astype(str)

train_df = train_df[(train_df['sentence1'].str.split().str.len() > 0) & (train_df['sentence2'].str.split().str.len() > 0)]
val_df = val_df[(val_df['sentence1'].str.split().str.len() > 0) & (val_df['sentence2'].str.split().str.len() > 0)]

train_df

val_df

import torch
from torch.utils.data import Dataset, TensorDataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import pickle
import os
from transformers import DebertaTokenizer, DebertaForSequenceClassification
#from transformers import AutoTokenizer, AutoModelForSequenceClassification
#from transformers import AlbertTokenizer

class MNLIDataDeberta(Dataset):

  def __init__(self, train_df, val_df):
    self.label_dict = {'entailment': 0, 'contradiction': 1, 'neutral': 2}

    self.train_df = train_df
    self.val_df = val_df

    self.base_path = '/content/'
    self.tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base', do_lower_case=True)
    #self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', do_lower_case=True)
    self.train_data = None
    self.val_data = None
    self.init_data()

  def init_data(self):
    # Saving takes too much RAM
    #
    # if os.path.exists(os.path.join(self.base_path, 'train_data.pkl')):
    #   print("Found training data")
    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'rb') as f:
    #     self.train_data = pickle.load(f)
    # else:
    #   self.train_data = self.load_data(self.train_df)
    #   with open(os.path.join(self.base_path, 'train_data.pkl'), 'wb') as f:
    #     pickle.dump(self.train_data, f)
    # if os.path.exists(os.path.join(self.base_path, 'val_data.pkl')):
    #   print("Found val data")
    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'rb') as f:
    #     self.val_data = pickle.load(f)
    # else:
    #   self.val_data = self.load_data(self.val_df)
    #   with open(os.path.join(self.base_path, 'val_data.pkl'), 'wb') as f:
    #     pickle.dump(self.val_data, f)
    self.train_data = self.load_data(self.train_df)
    self.val_data = self.load_data(self.val_df)

  def load_data(self, df):
    MAX_LEN = 512
    token_ids = []
    mask_ids = []
    seg_ids = []
    y = []

    premise_list = df['sentence1'].to_list()
    hypothesis_list = df['sentence2'].to_list()
    label_list = df['gold_label'].to_list()

    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):
      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)
      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)
      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]
      premise_len = len(premise_id)
      hypothesis_len = len(hypothesis_id)

      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1
      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values

      token_ids.append(torch.tensor(pair_token_ids))
      seg_ids.append(segment_ids)
      mask_ids.append(attention_mask_ids)
      y.append(self.label_dict[label])
    
    token_ids = pad_sequence(token_ids, batch_first=True)
    mask_ids = pad_sequence(mask_ids, batch_first=True)
    seg_ids = pad_sequence(seg_ids, batch_first=True)
    y = torch.tensor(y)
    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)
    print(len(dataset))
    return dataset

  def get_data_loaders(self, batch_size=32, shuffle=True):
    train_loader = DataLoader(
      self.train_data,
      shuffle=shuffle,
      batch_size=batch_size
    )

    val_loader = DataLoader(
      self.val_data,
      shuffle=shuffle,
      batch_size=batch_size
    )

    return train_loader, val_loader

mnli_dataset = MNLIDataDeberta(train_df, val_df)

train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=16)

from transformers import DebertaForSequenceClassification, AdamW

#model = AlbertForSequenceClassification.from_pretrained("albert-base-v2", num_labels=3)
model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-base', num_labels=3)
model.to(device)

param_optimizer = list(model.named_parameters())
no_decay = ['bias', 'gamma', 'beta']
optimizer_grouped_parameters = [
    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
     'weight_decay_rate': 0.01},
    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
     'weight_decay_rate': 0.0}
]

# This variable contains all of the hyperparemeter information our training loop needs
optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)

# for name, param in model.named_parameters():                
#     if name.startswith('albert'):
#         param.requires_grad = False

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')

def multi_acc(y_pred, y_test):
  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))
  return acc

import time

EPOCHS = 5

def train(model, train_loader, val_loader, optimizer):  
  total_step = len(train_loader)

  for epoch in range(EPOCHS):
    start = time.time()
    model.train()
    total_train_loss = 0
    total_train_acc  = 0
    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):
      optimizer.zero_grad()
      pair_token_ids = pair_token_ids.to(device)
      mask_ids = mask_ids.to(device)
      seg_ids = seg_ids.to(device)
      labels = y.to(device)
      # prediction = model(pair_token_ids, mask_ids, seg_ids)
      loss, prediction = model(pair_token_ids, 
                             token_type_ids=seg_ids, 
                             attention_mask=mask_ids, 
                             labels=labels).values()

      # loss = criterion(prediction, labels)
      acc = multi_acc(prediction, labels)

      loss.backward()
      optimizer.step()
      
      total_train_loss += loss.item()
      total_train_acc  += acc.item()

    train_acc  = total_train_acc/len(train_loader)
    train_loss = total_train_loss/len(train_loader)
    model.eval()
    total_val_acc  = 0
    total_val_loss = 0
    with torch.no_grad():
      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):
        optimizer.zero_grad()
        pair_token_ids = pair_token_ids.to(device)
        mask_ids = mask_ids.to(device)
        seg_ids = seg_ids.to(device)
        labels = y.to(device)

        # prediction = model(pair_token_ids, mask_ids, seg_ids)
        loss, prediction = model(pair_token_ids, 
                             token_type_ids=seg_ids, 
                             attention_mask=mask_ids, 
                             labels=labels).values()
        
        # loss = criterion(prediction, labels)
        acc = multi_acc(prediction, labels)

        total_val_loss += loss.item()
        total_val_acc  += acc.item()

    val_acc  = total_val_acc/len(val_loader)
    val_loss = total_val_loss/len(val_loader)
    end = time.time()
    hours, rem = divmod(end-start, 3600)
    minutes, seconds = divmod(rem, 60)

    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')
    print("{:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds))

train(model, train_loader, val_loader, optimizer)

import pandas as pd
import torch
import time
import random
import os

tokenizer = AutoTokenizer.from_pretrained("microsoft/deberta-xlarge-mnli")

best_match = {}
conv =[]
response =''
df = pd.read_csv('/content/drive/My Drive/datamainlist2.csv')
sdf = pd.read_csv('/content/drive/My Drive/sample_inputs.csv',encoding='latin-1')
conv=df.conversation
#print("Talk to me: ")
#inpt = input()


for index, row in sdf.iterrows():
    inpt=row["conversation"]
    data = []

    print("Retrieving the response.......")
    score=0
    count=0
    start=time.time()
    # logic to iterate through entire dataset
    for i in range(len(conv)):
        device = "cuda:0"
        model = model.to(device)
        paraphrase = tokenizer.encode_plus(inpt,conv[i], return_tensors="pt")
        paraphrase = paraphrase.to(device)
        paraphrase_classification_logits = model(**paraphrase)[0]
        paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]

        if(len(best_match)>0 and list(best_match.values())[0]< paraphrase_results[2]):
            best_match = {}
            best_match[i] = paraphrase_results[2]
        elif(len(best_match)==0):
            best_match[i] = paraphrase_results[2]
        count = count+1
        score = score+int(paraphrase_results[2]*100)


    avg_score=score/count
    print("##################################################")
    print("Full Dataset results:")
    print("Average Score for "+ inpt+": "+ str(avg_score))
    response=conv[list(best_match.keys())[0]]
    print('***********************************')
    print('Response:' + response)
    print('***********************************')
    end = time.time()
    data.append([inpt, 'Full Dataset', avg_score, list(best_match.values())[0], end-start ])
    print( 'Entailment score :' + str(list(best_match.values())[0]))
    print('Took ' +str(end-start)+' seconds to process the response with '+str(len(conv))+ ' entries')
    print("##################################################")
    print("                                                  ")


    score=0
    count=0
    best_match = {}
    start=time.time()
    # logic to pick random sentences (500 iterations) from the dataset and pick the best entailment score
    for j in range(0,500):
        i = random.randrange(len(conv))
        paraphrase = tokenizer.encode_plus(inpt, conv[i], return_tensors="pt")
        device = "cuda:0"
        model = model.to(device)
        paraphrase = paraphrase.to(device)
        paraphrase_classification_logits = model(**paraphrase)[0]
        paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]

        if(len(best_match)>0 and list(best_match.values())[0]< paraphrase_results[2]):
            best_match = {}
            best_match[i] = paraphrase_results[2]
        elif(len(best_match)==0):
            best_match[i] = paraphrase_results[2]
        count = count+1
        score = score+int(paraphrase_results[2]*100)


    avg_score=score/count
    print("##################################################")
    print("Random search for 500 interations")
    print("Average Score for "+ inpt+": "+ str(avg_score))
    response=conv[list(best_match.keys())[0]]
    print('***********************************')
    print('Response:' + response)
    print('***********************************')
    end = time.time()
    data.append([inpt, 'Random search with 500 iterations', avg_score, list(best_match.values())[0], end-start ])
    print( 'Entailment score :' + str(list(best_match.values())[0]))
    print('Took ' +str(end-start)+' seconds to process the response with '+str(len(conv))+ ' entries')
    print("##################################################")
    print("                                                  ")





    score=0
    count=0
    best_match = {}
    start=time.time()
    # logic to iterate on entire dataset but break in case it finds an entailment score higher than the threshold
    for i in range(len(conv)):
        paraphrase = tokenizer.encode_plus(inpt, conv[i], return_tensors="pt")
        device = "cuda:0"
        model = model.to(device)
        paraphrase = paraphrase.to(device)
        paraphrase_classification_logits = model(**paraphrase)[0]
        paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]

        if(len(best_match)>0 and list(best_match.values())[0]< paraphrase_results[2]):
            best_match = {}
            best_match[i] = paraphrase_results[2]
        elif(len(best_match)==0):
            best_match[i] = paraphrase_results[2]
        count = count+1
        score = score+int(paraphrase_results[2]*100)
        if(int(paraphrase_results[2]) > 50):
            break
    avg_score=score/count
    print("##################################################")
    print("Full Dataset with a threshold limit:")
    print("Average Score for "+ inpt+": "+ str(avg_score))
    response=conv[list(best_match.keys())[0]]
    print('***********************************')
    print('Response:' + response)
    print('***********************************')
    end = time.time()
    data.append([inpt, 'Full Dataset with Threshold at 50', avg_score, list(best_match.values())[0], end-start ])
    print( 'Entailment score :' + str(list(best_match.values())[0]))
    print('Took ' +str(end-start)+' seconds to process the response with '+str(len(conv))+ ' entries')
    print("##################################################")
    print("                                                  ")



    df = pd.DataFrame(data, columns=["Input", "Type", "avg_score", "entailment_score", "time_taken"])
    if not os.path.isfile('/content/drive/My Drive/metrics.csv'):
       df.to_csv('/content/drive/My Drive/metrics.csv', header=["Input", "Type", "avg_score", "entailment_score", "time_taken" ])
    else:
       df.to_csv('/content/drive/My Drive/metrics.csv', mode='a', header=False)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

entailments = pd.read_csv('/content/drive/My Drive/metrics.csv')

sns.lmplot('avg_score', 'time_taken', data=entailments, hue='Type', fit_reg=False)
sns.lmplot('entailment_score', 'time_taken', data=entailments, hue='Type', fit_reg=False)
sns.lmplot('avg_score','entailment_score', data=entailments, hue='Type', fit_reg=False)
